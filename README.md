
## Working with Pyspark
Apache Spark is an open source framework that allows to perform large scale data processing at good speed. PySpark is an Python API of Apache Spark  which acts as an interface to work on Spark using Python.It was mostly written in Scala programming language. It runs 100x faster than the Hadoop-Mapreduce with advent of "In-memory processing" and supports real-time computation.It provides robust, fault tolerant data objects named as RDD's that are useful to work on Machine learning and big data based applications.

#### PySpark Using Colab
To work with PySpark on local machine needs Java, Scala, Py4j library etc and several other software to be installed so instead PySpark on Google Colab is better alternative when the data used is mounted on drive.
#### RDD - Resilient Distributed Datasets
